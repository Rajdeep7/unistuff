{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) \n",
    "We need to find a good estimate for the parameters / weights. Since we only have one input feature $x$ and one target $t$, we have two weights $w_0$ and $w_1$ with $f(x, \\mathbf{w}) = w_0 + w_1x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) \n",
    "The optimal point estimate for $\\mathbf{w}$ would be the one with the least sum of squared errors. This can be found by setting the gradient of the sum of squared errors $E$ to zero:\n",
    "\n",
    "$\\nabla E(\\mathbf{w}) = \\sum_i \\mathbf{w}^T \\mathbf{x}_i \\mathbf{x}_i^T - \\sum_i t_i \\mathbf{x}_i^T = (0, 0)$\n",
    "\n",
    "with $\\mathbf{x}_i = (1, x_i)^T$ and $\\mathbf{w} = (w_0, w_1)^T$. This yields\n",
    "\n",
    "$\\mathbf{w}^T \\sum_i \\mathbf{x}_i \\mathbf{x}_i^T = \\sum_i t_i \\mathbf{x}_i^T$\n",
    "\n",
    "The sums can be calculated independently from $\\mathbf{w}$, so we can rewrite the equation above to\n",
    "\n",
    "$(w_0, w_1) \\begin{bmatrix}a_{1,1} & a_{1,2}\\\\a_{2,1} & a_{2,2}\\end{bmatrix} = (b_1, b_2)$\n",
    "\n",
    "This system of linear equations can easily be solved yielding the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c)\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "**Assumptions**:\n",
    "1. $p(x)$ is the same for all $x$\n",
    "2. The residuals $(t - f(x, \\mathbf{w}))$ are normally distributed\n",
    "  1. The mean of this distribution is zero\n",
    "  2. The variance $\\sigma^2$ of this distribution is fixed. It does not depend on $\\mathbf{w}$ or $x$.\n",
    "  \n",
    "$\\mathbf{w}^* = \\underset{w}{\\text{argmax}} \\sum_i \\log p(t_i | x_i, \\mathbf{w})$\n",
    "\n",
    "with $p(t_i | x_i, \\mathbf{w}) = \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp(-\\frac{(t_i - f(x_i, \\mathbf{w}))^2}{\\sigma^2})$\n",
    "\n",
    "$\\mathbf{w}^* = \\underset{w}{\\text{argmax}} \\sum_i  \\Big[\\log \\dfrac{1}{\\sqrt{2\\pi \\sigma^2}} - \\frac{(t_i - f(x_i, \\mathbf{w}))^2}{\\sigma^2}\\Big]$\n",
    "\n",
    "Since $\\sigma^2$ is fixed, we get\n",
    "\n",
    "$\\mathbf{w}^* = \\underset{w}{\\text{argmin}} \\sum_i (t_i - f(x_i, \\mathbf{w}))^2$\n",
    "\n",
    "which means we need to minimize the Sum of Squared Errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum A-Posteriori Estimation\n",
    "\n",
    "**Assumptions**:\n",
    "1. $p(x)$ is the same for all $x$\n",
    "2. The residuals $(t - f(x, \\mathbf{w}))$ are normally distributed\n",
    "  1. The mean of this distribution is zero\n",
    "  2. The variance $\\sigma^2$ of this distribution is fixed. It does not depend on $\\mathbf{w}$ or $x$.\n",
    "3. The weights are normally distributed: $\\mathbf{w} \\sim \\mathcal{N}((0, 0)^T, \\mathbb{I}_2)$\n",
    "\n",
    "$\\mathbf{w}^* = \\underset{w}{\\text{argmax}} \\sum_i \\Big[\\log p(t_i | x_i, \\mathbf{w}) + \\log p(\\mathbf{w})\\Big]$\n",
    "\n",
    "$= \\underset{w}{\\text{argmin}} \\frac{1}{N} \\sum_i (t_i - f(x_i, \\mathbf{w}))^2 - \\log p(\\mathbf{w})$\n",
    "\n",
    "$= \\underset{w}{\\text{argmin}} \\frac{1}{N} \\sum_i (t_i - f(x_i, \\mathbf{w}))^2 - \\lambda \\lVert \\mathbf{w} \\rVert^2_2$\n",
    "\n",
    "for a fixed $\\lambda$, which means we need to minimize the Sum of Squared Errors and the L2 penalty on the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Inference\n",
    "\n",
    "**Assumptions**:\n",
    "1. $p(x)$ is the same for all $x$\n",
    "2. The residuals $(t - f(x, \\mathbf{w}))$ are normally distributed\n",
    "  1. The mean of this distribution is zero\n",
    "  2. The variance $\\sigma^2$ of this distribution is fixed. It does not depend on $\\mathbf{w}$ or $x$.\n",
    "3. The weights are normally distributed: $\\mathbf{w} \\sim \\mathcal{N}((0, 0)^T, \\mathbb{I}_2)$\n",
    "\n",
    "With our dataset $D = \\{x_i, t_i\\}_{i=1...N}$, we calculate the posterior \n",
    "\n",
    "$p(\\mathbf{w}|D) = \\dfrac{p(D|\\mathbf{w}) \\cdot p(\\mathbf{w})}{p(D)} = \\dfrac{p(D|\\mathbf{w}) \\cdot p(\\mathbf{w})}{\\int p(D|\\mathbf{w}) \\cdot p(\\mathbf{w}) d\\mathbf{w}}$\n",
    "\n",
    "with $p(D|\\mathbf{w}) = \\prod_i p(t|x,\\mathbf{w})$. \n",
    "\n",
    "$p(t|x,\\mathbf{w})$ and $p(\\mathbf{w})$ are defined as above. \n",
    "\n",
    "We can then use this posterior for predicting $t'$ for $x'$:\n",
    "\n",
    "$p(t'|x', D) = \\int p(t'|x', \\mathbf{w}) \\cdot p(\\mathbf{w} | D) d \\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.array([\n",
    "    [2, 0, 1],\n",
    "    [1.08, 1.68, 2.38],\n",
    "    [-0.83, 1.82, 2.49],\n",
    "    [-1.97, 0.28, 2.15],\n",
    "    [-1.31, -1.51, 2.59],\n",
    "    [0.57, -1.91, 4.32]\n",
    "])\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.plot(x[:,0], x[:,1], x[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance per time interval\n",
    "deltas = x[1:] - x[:-1]\n",
    "print('deltas', deltas)\n",
    "\n",
    "distances = np.linalg.norm(deltas, axis=1)\n",
    "print('distances', distances)\n",
    "\n",
    "# Calculate the least squares fit for the data\n",
    "\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. In the Bayesian approach to linear regression we have \n",
    "  \n",
    "  $p(w|x,t) = \\dfrac{p(x, t|w) \\cdot p(w)}{p(x, t)}$\n",
    "  \n",
    "  However, the slides use \n",
    "  \n",
    "  $p(w|x,t) = \\dfrac{p(t|x, w) \\cdot p(w)}{p(t|x)}$\n",
    "  \n",
    "  Since \n",
    "  \n",
    "  $p(x,t|w) = p(t|x,w) \\cdot p(x|w) = p(t|x,w) \\cdot p(x)$ \n",
    "  \n",
    "  do the slides implicitly assume that $p(x)$ is the same for all $x$?\n",
    "    \n",
    "2. Does MLE / MAP count as \"a probabilistic way\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
